<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="description" content="" />
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
        <script defer src="./static/js/fontawesome.all.min.js"></script>
        <link
            rel="stylesheet"
            href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" />
        <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.1.0/prism-bibtex.min.js"></script>
        <link rel="stylesheet" href="static/css/index.css" />

        <title>Multistep Quasimetric Learning for Scalable Goal-conditioned Reinforcement Learning</title>

        <script
            type="text/javascript"
            id="MathJax-script"
            async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-svg.js"></script>
        <script>
            function selectContent(query) {
                var range = document.createRange()
                var selection = window.getSelection()
                var elem = document.querySelector(query)
                range.selectNodeContents(elem)
                selection.removeAllRanges()
                selection.addRange(range)
            }
        </script>
    </head>

    <body>
        <script type="text/javascript">
            window.MathJax = {
                tex: {
                    inlineMath: [
                        ["$", "$"],
                        ["\\(", "\\)"],
                    ],
                    displayMath: [
                        ["$$", "$$"],
                        ["\\[", "\\]"],
                    ],
                    processEscapes: true,
                    macros: {
                        S: "\\mathcal{S}",
                        A: "\\mathcal{A}",
                        D: "\\mathcal{D}",
                        R: "\\mathbb{R}",
                        E: "\\mathbb{E}",
                        P: "\\mathrm{P}",
                        var: "\\mathrm{Var}",
                        cov: "\\mathrm{Cov}",
                        argmin: "\\mathop{\\arg\\min}",
                        argmax: "\\mathop{\\arg\\max}",
                    },
                },
                svg: {
                    fontCache: "global",
                },
            }
        </script>
        <header>
            <h1>Multistep Quasimetric Learning for Scalable Goal-conditioned Reinforcement Learning</h1>
            <div class="authors">
                <span class="author"><a href="https://wj2003b.github.io" target="_blank">Bill Zheng</a></span>
                <span class="affil">1</span>
                <span class="author"><a href="https://people.eecs.berkeley.edu/~vmyers/" target="_blank">Vivek Myers</a></span>
                <span class="affil">1</span>
                <span class="author"><a href="https://ben-eysenbach.github.io/" target="_blank">Benjamin Eysenbach</a></span>
                <span class="affil">2</span>
                <span class="author"><a href="https://people.eecs.berkeley.edu/~svlevine/" target="_blank">Sergey Levine</a></span>
                <span class="affil">1</span>


            </div>
            <div class="notes">
                <span class="affil">1</span>
                <span class="institution">UC Berkeley</span>
                <span class="affil">2</span>
                <span class="institution">Princeton University</span>
            </div>
            <div class="links">
                <span class="link">
                    <a href="./static/pdf/mqe-paper.pdf" target="_blank" class="button">
                        <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
                    </a>
                </span>
                <span class="link">
                    <a href="https://anonymous.4open.science/r/mqe-paper-686D/" class="button">
                        <span class="icon"><i class="fas fa-code"></i></span><span>Code</span>
                    </a>
                </span>
            </div>
        </header>

        <main>
            <!-- <section>
                <div class="summary">
                    <img src="static/figures/main_fig.svg" alt="Overview" />
                </div>
            </section> -->

            <section>
                <div>
                    <div class="wide margin abstract">
                        <h3>Abstract</h3>
                        <p>
                            Learning how to reach goals in an environment is a longstanding challenge in AI, yet reasoning over long horizons remains a challenge for modern methods. The key question is how to estimate the temporal distance between pairs of observations. While temporal difference methods leverage local updates to provide optimality guarantees, they often perform worse than Monte Carlo methods that perform global updates (e.g., with multi-step returns), which lack such guarantees. We show how these approaches can be integrated into a practical GCRL method that fits a quasimetric distance using a multistep Monte-Carlo return. We show our method outperforms existing GCRL methods on long-horizon simulated tasks with up to 4000 steps, even with visual observations. We also demonstrate that our method can enable stitching in the real- world robotic manipulation domain (Bridge setup). Our approach is the first end-to-end GCRL method that enables multistep stitching in this real-world manipulation domain from an unlabeled offline dataset of visual observations.
                        </p>
                    </div>

                </div>
            </section>

            <section>
                <div>
                    <h3>Motivation</h3>
                    <div style="text-align: center;">
                            <img src="static/figures/mainfig.svg" style="width:80%;" />
                    </div>
                    <p>In goal-conditioned off-policy RL, it is a highly nontrivial task to perform task-stitching: imagine you have seen two separate tasks in a training dataset, and you want to combine them together in evaluation. How should we do that? Recently, quasimetric architecture for value learning has demonstrated. Multistep Quasimetric Estimation (MQE) addresses the following challenges:</p>
                <ol>
                    <li>How can we scale up the horizon generalization capabilities of value learning using quasimetric distance representations?</li>
                    <li>How can we provide a good quasimetric method for real-world robotic manipulation tasks?</li>
                </ol>
                </div>
            </section>

            <section>
                <div>
                    <h3>Method</h3>
                    <p>The main idea of MQE is that to incorporate multistep backup... </p>
                    <details>
                        <summary style="cursor: pointer; font-size: 1em; margin: 0.5em 0;">
                            Show pseudocode
                        </summary>
                        <img src="static/figures/method.png" style="margin-top:1em; max-width:100%;" />
                    </details>
                </div>
            </section>

            <section>
                <div>
                    <h3>Experiments</h3>
                    
                    <h4>OGBench Evaluation</h4>
                    <div style="display: flex; justify-content: center; align-items: flex-start; gap: 20px; margin-bottom: 1em;">
                        <div style="display: flex; align-items: flex-start; gap: 20px; width: 100%;">
                            <div style="flex: 1; min-width: 200px;">
                                <p>
                                    <ul>
                                        <li>MQE achieves the strongest performance against a variety of offline RL methods on OGBench (90 tasks in total) across both state and pixel-based observations.</li>
                                        <li>In particular, MQE outperforms both comparable quasimetric value learning methods (QRL, CMD, TMD) as well as horizon-reduction methods (n-SAC+BC, HIQL) <i>without requiring any hierarchical components</i>!</li>
                                    </ul>
                                </p>
                            </div>
                            <img src="static/figures/results-bar.png" alt="Main Results Bar Chart" style="flex: 1; max-width: 48%; height: auto; border: none; border-radius: 5px; margin-left: 0;">
                        </div>
                    </div>
                    <details>
                    <summary style="cursor: pointer; font-size: 1em; margin: 0.5em 0;">
                        Show Complete Results Table
                    </summary>
                    <img src="static/figures/ogbench.png" />
                    </details>
                    <h4>OGBench Rollouts</h4>
                    <video src="https://res.cloudinary.com/dp7qzzmt2/video/upload/v1760397472/scene_rh6jgn.mp4" controls></video>
                    <h4>Real-World Evaluation</h4>
                </div>
            </section>

            <section>
                <div>
                    <h3>Ablations</h3>
                    
                    <h4>Can we get a better distance representation compared to other RL methods?</h4>
                    <p>Let's take a look at what are the distances that we actually learn from these methods. We use <span style="font-family: 'American Typewriter', 'Courier New', Courier, monospace;">antmaze-large-explore</span> to find the learned distances $d(s, g)$. As the trajectories inside the dataset is highly noisy, we can use this to empirically compare how well our value functions are being distilled.</p>
                    <figure style="text-align:center;">
                        <img src="static/figures/distances.png" alt="Learned distances figure" style="display:block; margin-left:auto; margin-right:auto;">
                        <figcaption style="font-size:0.95em; margin-top:0.5em;">
                            Distance visualization with several offline RL methods.
                        </figcaption>
                    </figure>
                    <p>From these figures</p>
                    <h4>OGBench Rollouts</h4>
                    <video src="https://res.cloudinary.com/dp7qzzmt2/video/upload/v1760397472/scene_rh6jgn.mp4" controls></video>
                    <h4>Real-World Evaluation</h4>
                </div>
            </section>

        </main>
    </body>
</html>
